# -------------------------------
# Import Libraries
# -------------------------------
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
from scipy.sparse import issparse

# -------------------------------
# Load Dataset
# -------------------------------
df = pd.read_csv("fake_job_postings.csv")

# -------------------------------
# Data Cleaning
# -------------------------------
categorical_cols = ['location', 'department', 'employment_type', 'function',
                    'industry', 'required_education', 'required_experience']
for col in categorical_cols:
    df[col] = df[col].fillna("Unknown").astype(str)

text_cols = ['title', 'description', 'company_profile', 'requirements', 'benefits']
for col in text_cols:
    df[col] = df[col].fillna("").str.strip().str.lower()

numeric_features = ['telecommuting', 'has_company_logo', 'has_questions']
for col in numeric_features:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# Feature Engineering
df['title_length'] = df['title'].apply(lambda x: len(str(x).split()))
df['description_length'] = df['description'].apply(lambda x: len(str(x).split()))
df['requirements_length'] = df['requirements'].apply(lambda x: len(str(x).split()))

engineered_features = ['title_length', 'description_length', 'requirements_length']

# -------------------------------
# Features & Labels
# -------------------------------
X = df.drop('fraudulent', axis=1)
y = df['fraudulent']

# -------------------------------
# Preprocessing Pipeline
# -------------------------------
categorical_features = categorical_cols
numeric_features = ['telecommuting', 'has_company_logo', 'has_questions']
text_features = ['title', 'description', 'requirements', 'company_profile']

preprocessor = ColumnTransformer(
    transformers=[
        ('title_tfidf', TfidfVectorizer(max_features=2000, stop_words='english'), 'title'),
        ('desc_tfidf', TfidfVectorizer(max_features=5000, stop_words='english'), 'description'),
        ('req_tfidf', TfidfVectorizer(max_features=2000, stop_words='english'), 'requirements'),
        ('profile_tfidf', TfidfVectorizer(max_features=1000, stop_words='english'), 'company_profile'),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('num', 'passthrough', numeric_features),
        ('eng', StandardScaler(), engineered_features)
    ],
    remainder='drop'
)

# -------------------------------
# Train/Test Split
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# -------------------------------
# Pipeline with SMOTE + Classifier
# -------------------------------
clf = RandomForestClassifier(
    n_estimators=300,
    max_depth=25,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)

model_pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', clf)
])

# Train pipeline (preprocessing + SMOTE + classifier)
model_pipeline.fit(X_train, y_train)
print("Pipeline trained successfully!")

# -------------------------------
# Evaluation Function
# -------------------------------
def evaluate_model(pipeline, X, y, dataset_name=""):
    y_proba = pipeline.predict_proba(X)[:, 1]
    y_pred = (y_proba >= 0.5).astype(int)

    print(f"\n--- {dataset_name} EVALUATION ---")
    print("Accuracy:", accuracy_score(y, y_pred))
    print("Balanced Accuracy:", balanced_accuracy_score(y, y_pred))
    print("ROC-AUC:", roc_auc_score(y, y_proba))
    print("PR-AUC:", average_precision_score(y, y_proba))
    print("\nClassification Report:\n", classification_report(y, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y, y_pred))

# Evaluate on Train & Test
evaluate_model(model_pipeline, X_train, y_train, "TRAIN SET")
evaluate_model(model_pipeline, X_test, y_test, "TEST SET")

# -------------------------------
# Save Pipeline
# -------------------------------
joblib.dump(model_pipeline, "model/fraud_job_pipeline.pkl")
print("Pipeline saved as 'model/fraud_job_pipeline.pkl'. Ready for Streamlit!")
